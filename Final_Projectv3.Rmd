---
title: "Final Project: Midterm Election Results - Sentiment Analysis Using Twitter's API"
author: "B. Sosnovski, E. Azrilyan and R. Mercier"
date: "11/23/2018"
output:
  prettydoc::html_pretty:
    theme: architect
    highlight: github
    section: TRUE
---

```{r, echo=TRUE, results='asis'}
#knitr::kable(head(iris, 10))
```

## Project Description: 

Sentiment analysis on the primary election results using Twitter data.

**Members:** B. Sosnovski, E. Azrilyan and R. Mercier

**Motivation:** Sentiment analysis plays an important role during elections. Political strategists can use the public’s opinions, emotions, attitudes, etc., to convert them into votes in 2020.
Can we use the public’s sentiment to determine which political party has the upper hand in the next election?
Data: To conduct our analysis we will harvest data using Twitter and Facebook’s APIs. Data will be restricted to a certain date range and geographic region. 

**Work Flow:** 

1. Fetch, clean and tokenize the data.
2. Perform feature selection to keep only the n-grams (most likely bigrams) that are meaningful for an analysis.
3. Classify results as positive, negative and neutral. At this phase of the project, we not sure yet what type of analysis to perform. A possible analysis that can be performed is one of the  following:

   + Counting and correlating pairs of words.
   + Build predictive models using logistic regression to predict the probability of occurrence of an event.
   + Probabilistic topic model using Latent Dirichlet Allocation (LDA) method to be applied for reading general tendency from FB/Twitter posts or comments into certain topics that can be classified toward positive and negative sentiment.
   
**Tools:**

+ Tweeter Premium Search API - 30-day endpoint (Sandbox), which provides Tweets from the previous 30 days.
+ R Packages

## Load Libraries
```{r}
library(twitteR)
library(rjson)
library(RMySQL)
library(httr)
```

## API's Information

Read key, key secret, access token and and access token secret from a text file to mantain the information confidential.

```{r}
api <- read.table("Twitter_API_Key.txt", header = TRUE, stringsAsFactors = FALSE)
names(api)
dim(api)
App_Name <- api$app_name
Consumer_Key <- api$key
Consumer_Secret <- api$secret_Key
Access_Token <- api$access_token
Access_Secret <- api$access_token_secret
```

Using access token method: create token and save it as an environment variable.
```{r}
setup_twitter_oauth(Consumer_Key, Consumer_Secret, Access_Token, Access_Secret)

```

## Data Acquisition

```{r}
## Collect tweets
tweets1 <- searchTwitter("#maxrose+#dandonovan", n=10000,lang = "en", since='2018-11-05', until='2018-11-09', retryOnRateLimit = 1000)
df <- twListToDF(tweets1)
write.csv(df, "tweets1.csv", row.names=FALSE)

## Back up tweets to a MySQL database
register_mysql_backend('tweets1_db', host = 'localhost', user='test_user', password='data607')
store_tweets_db(tweets1)

## Load tweets from database
from_db = load_tweets_db(as.data.frame = TRUE)
```

## Tweets Preprocessing 

## Tweets Cleaning
 
The next steps is to  cleanse the texts in the tweets by:
 
* Removing Twitter handles (@user)
* Removing punctuations, numbers, and special characters 
* Removing white spaces (?) and stop words
* Remove hashtags, tags, urls, Twitter short words, etc.
* Converting the corpus to lower case (?)

## Sentiment Analysis

Check packages SentimentAnalysis and sentimentr.

## Reference

Jeff Gentry. March 18, 2014. "twitteR - Twitter client for R."  R package version 1.1.9. <https://www.rdocumentation.org/packages/twitteR/versions/1.1.9>

library(knitr)
library(XML)
library(RCurl)
library(jsonlite)
library(methods)
library(tm)
library(twitteR)

#Creating data frames for the individual Twitter stream files; including UTF-8.
BS1.1 <- url("https://raw.githubusercontent.com/bsosnovski/FinalProject/e42c792ab9233f182c5a8e007eed5b0aef85a1ce/tweets.csv")
Stream1 <- read.csv(BS1.1,header = TRUE, fileEncoding = "ASCII")
Stream1.1 <- Stream1[c(1,4,31)]

BS1.2 <- url("https://raw.githubusercontent.com/bsosnovski/FinalProject/e42c792ab9233f182c5a8e007eed5b0aef85a1ce/tweets2.csv")
Stream2 <- read.csv(BS1.2,header = TRUE, fileEncoding = "ASCII")
Stream2.1 <- Stream2[c(1,4,29)]

BS1.3 <- url("https://raw.githubusercontent.com/bsosnovski/FinalProject/e42c792ab9233f182c5a8e007eed5b0aef85a1ce/tweets3.csv")
Stream3 <- read.csv(BS1.3,header = TRUE, fileEncoding = "ASCII")
Stream3.1 <- Stream3[c(1,4,29)]

BS1.4 <- url("https://raw.githubusercontent.com/bsosnovski/FinalProject/e42c792ab9233f182c5a8e007eed5b0aef85a1ce/tweets4.csv")
Stream4 <- read.csv(BS1.4,header = TRUE, fileEncoding = "ASCII")
Stream4.1 <- Stream4[c(1,4,29)]

BS1.5 <- url("https://raw.githubusercontent.com/bsosnovski/FinalProject/e42c792ab9233f182c5a8e007eed5b0aef85a1ce/tweets5.csv")
Stream5 <- read.csv(BS1.5,header = TRUE, fileEncoding = "ASCII")
Stream5.1 <- Stream5[c(1,4,31)]

BS1.6 <- url("https://raw.githubusercontent.com/bsosnovski/FinalProject/master/tweets6.csv")
Stream6 <- read.csv(BS1.6,header = TRUE, fileEncoding = "ASCII")
Stream6.1 <- Stream6[c(1,4,30)]

Stream <- rbind(Stream1.1, Stream2.1, Stream3.1, Stream5.1, Stream6.1)
combined_doc <- iconv(Stream, "UTF-8", "ASCII", sub = "")
combined_doc

### Check on #2
Mycorpus <- Corpus(VectorSource(Stream$results.text))
#Various cleansing funtions:
#ASCII Symbols
remove_ASCIIs <- function(x) gsub("[^\x01-\x7F]", "", x)
Mycorpus <- tm_map(Mycorpus, content_transformer(remove_ASCIIs))
#Retweets
remove_RTs <- function(x) gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", x)
Mycorpus <- tm_map(Mycorpus, content_transformer(remove_RT))
#@'s 
remove_ATs <- function(x) gsub("@\\w+", "", x)
Mycorpus <- tm_map(Mycorpus, content_transformer(remove_ATs))
#All Punctuations
remove_Puncts <- function(x) gsub("[[:punct:]]", "", x)
Mycorpus <- tm_map(Mycorpus, content_transformer(remove_Puncts))
#All Digits
remove_Digits <- function(x) gsub("[[:digit:]]", "", x)
Mycorpus <- tm_map(Mycorpus, content_transformer(remove_Digits))
#3-Step HTTP Process
remove_HTTPSs <- function(x) gsub("http\\w+", "", x)
Mycorpus <- tm_map(Mycorpus, content_transformer(remove_HTTPSs))
remove_HTTPSs2 <- function(x) gsub("[ \t]{2,}", "", x)
Mycorpus <- tm_map(Mycorpus, content_transformer(remove_HTTPSs2))
remove_HTTPSs3 <- function(x) gsub("^\\s+|\\s+$", "", x)
Mycorpus <- tm_map(Mycorpus, content_transformer(remove_HTTPSs3))

#Whitespaces
remove_WhiteSpace <- function(x) gsub("[ \t]{2,}", "", x)
Mycorpus <- tm_map(Mycorpus, content_transformer(remove_WhiteSpace))
#stopwards
Mycorpus <- tm_map(Mycorpus, removeWords,stopwords("en"))
#Lower Case
Mycorpus <- tm_map(Mycorpus, content_transformer(tolower))

dtm <- DocumentTermMatrix(Mycorpus)


wordcloud(Mycorpus, random.order=F, scale=c(3, 0.5), min.freq = 5, col=rainbow(50))

