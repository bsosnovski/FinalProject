---
title: "Final Project: Midterm Election Results - Sentiment Analysis Using Twitter's API"
author: "B. Sosnovski, E. Azrilyan and R. Mercier"
date: "11/23/2018"
output:
  prettydoc::html_pretty:
    theme: architect
    highlight: github
    section: TRUE
---

# Project Description: 

Using Twitter API for sentiment analysis on the 2018 midterm election results for the 11th Congressional District, which includes all of Staten Island and parts of Southern Brooklyn.

**Members:** B. Sosnovski, E. Azrilyan and R. Mercier

**Motivation:** Sentiment analysis plays an important role during elections. Political strategists can use the public’s opinions, emotions, and attitudes to convert those into votes in 2020.

Can we gauge the public’s sentiment related to the results of the midterm election to a particular?

Data: To conduct our analysis we will harvest data using one of the Twitter's API. Data will be restricted to a certain date range for a NY's 11th Congressional District election race. 

**Work Flow:** 

1. Fetch, clean and tokenize the data.
2. Perform feature selection to keep only the tweets that are meaningful for the analysis.
3. Classify results as positive or negative.

Our collection of texts from tweets can be divided into natural groups so that we can understand them separately. Topic modeling is a method for unsupervised classification of such documents, which finds natural groups of items. 

We will perform a probabilistic topic model using Latent Dirichlet Allocation (LDA). LDA can be applied for reading general tendency from Twitter posts or comments into certain topics that can be classified toward positive and negative sentiment.

We also use the statistic TF-IDF (Term Frequency and Inverse Document Frequency) in our analysis, which attempts to find the words that are important (i.e., common) in a text, but not too common.

   
**Tools:**

+ Tweeter Premium Search API - 30-day endpoint (Sandbox), which provides tweets from the previous 30 days.
+ R Packages

# Twitter Premium API

First we needed to obtain the Twitter Premium API access. The following steps were taken to set up a twitter account and be able to use the Twitter API.

1. Created a Twitter account.
2. Logged in with the twitter credentials on <https://dev.twitter.com/apps> and applied for a developer account.
3. After receiving approval from Twitter, submitted an application to create new app, filled out the form and agreed to the terms.
4. Created the Keys and Access Tokens.

![Twitter Dashboard for Developers ](https://i.imgur.com/Sd2QMvj.png){ width=75% }

# Load Libraries
```{r}
library(twitteR)
library(httr)
library(base64enc)
library(jsonlite)
library(stringr)
suppressMessages(library(tidyverse))
library(tidytext)
library(knitr)
library(XML)
suppressMessages(library(RCurl))
library(methods)
suppressMessages(library(tm))
suppressMessages(library(wordcloud))
library(topicmodels)
```

# API Credentials

Read key, key secret, access token and and access token secret from a text file to mantain the information confidential.

```{r eval=FALSE}
api <- read.table("Twitter_API_Key.txt", header = TRUE, stringsAsFactors = FALSE)
names(api)
dim(api)
App_Name <- api$app_name
Consumer_Key <- api$key
Consumer_Secret <- api$secret_Key
Access_Token <- api$access_token
Access_Secret <- api$access_token_secret
```


# API Authentication

We faced a challenge in this part of the project because much of the documentation available on how to access the Twitter APIs using R is about accessing the Twitter's Basic Search API and not the Premium Search API, which was launched in Nov 2017 and is relatively new to the community. The basic Twitter API only gives access to the previous 7 days of posted tweets. To conduct our analysis, we needed access to tweets posted earlier than prior 7 days.

The following chunk of code was retrieved from <https://twittercommunity.com/t/how-to-use-premium-api-for-the-first-time-beginner/105346/10>. This was the only mention that we could find about this.

```{r eval=FALSE}
# base64 encoding
kands <- paste(Consumer_Key, Consumer_Secret, sep=":")
base64kands <- base64encode(charToRaw(kands))
base64kandsb <- paste("Basic", base64kands, sep=" ")

# request bearer token
resToken <- POST(url = "https://api.twitter.com/oauth2/token",
                 add_headers("Authorization" = base64kandsb, "Content-Type" = "application/x-www-form-urlencoded;charset=UTF-8"),
                 body = "grant_type=client_credentials")

# get bearer token
bearer <- content(resToken)
bearerToken <- bearer[["access_token"]]
bearerTokenb <- paste("Bearer", bearerToken, sep=" ")
```

# Data Acquisition

Since the Twitter Premium API - Sandbox (free version) limits access to the tweets posted for the last 30 days only, it is important to save the results of the search into csv files. This way, we can have access to the results afterwards, even when the data was no longer available via the API.

When converting the data received from the API  to a data frame, it may include columns with lists as observations. In this case, the R function "write.csv" or "write_csv" return an error. The function below identifies which columns of the data frame contain lists.  The information removed in the process is not important for our project.   

```{r}
# Function to identify which columns are lists
list_col <- function(df){
        n <- length(df)
        vec <- vector('numeric')
        for (i in 1:n){
                cl <- df[,i]
                if(class(cl)=="list"){
                        vec <- c(vec,i)
                }
        }
        return(vec)
}

```

Requests for data will likely generate more data than can be returned in a single response (our limit is up to 100 tweets in a single response). When a response is paginated, the API response will respond with a "Next" token specified in the body of the response that indicates whether any further pages are available. These "Next" page tokens can then be used to make further requests.The following function automates the API requests with or without pagination. 

The code below requests tweets which include the following search terms from 2018/11/05 to 2018/11/19:

- "#maxrose"

- "#dandonovan"

- "@RepDanDonovan"

- "@MaxRose4NY"


```{r eval=FALSE}

# the query includes terms "#maxrose", "#dandonovan", "@RepDanDonovan", "@MaxRose4NY" 
# data range: from 2018/11/05 to 2018/11/19

sbody = "{\"query\": \"#maxrose OR #dandonovan OR @RepDanDonovan OR @MaxRose4NY\",\"maxResults\": 100, \"fromDate\":\"201811050000\", \"toDate\":\"201811190000"
ebody = "\"}"

request <- function(start_body,end_body){
         full_body <- str_c(start_body, end_body, sep = "")
         nxt <-""
         pageNo <- 1
         
         while(!is.null(nxt)){
                resTweets <- POST(url = "https://api.twitter.com/1.1/tweets/search/30day/dev.json",
                  add_headers("authorization" = bearerTokenb, "content-Type" = "application/json"),
                  body = full_body)
                
                #checking if the type of response is json
                if (http_type(resTweets) != "application/json") {
                        stop("API did not return json", call. = FALSE)}
                
                #checking if the resquest was successful
                if (http_error(resTweets)) {
                        stop(sprintf("Twitter API request failed! Status = %s. Check what went wrong.\n", 
                                     status_code(resTweets)),
                             call. = FALSE)}else{
                                     message("Retrieving page ",pageNo)}
                
                # Parse the data
                tweets_df <- fromJSON(content(resTweets, "text"),flatten = TRUE) %>% data.frame()
        
                # Saving data only from the texts of the tweets in separate files
                text_df <- tweets_df$results.text
                file1 <- str_c("text",pageNo,".csv")
                write.csv(text_df, file1, row.names=FALSE)
        
                # Remove the list-columns
                vec<-list_col(tweets_df)
                tweets_df <- tweets_df[,-vec]

                # Saving the whole data
                file2 <- str_c("tweet",pageNo,".csv")
                write.csv(tweets_df, file2, row.names=FALSE)
        
                # Read the "next"" token received in the response
                nxt <- tweets_df$next.[[1]]
        
                if(!is.null(nxt)){
                        # insert the next token in the body of the request
                        full_body <- str_c(start_body, "\", \"next\":\"", nxt, end_body, sep = "")
                        pageNo <- pageNo+1}
                
                # To avoid to exceed the API's limit per minute
                Sys.sleep(3)
        } #end of while loop
         
}#end of function

request(start_body = sbody,end_body = ebody)
```

The screenshot below shows Twitter API in action, the following message appears for every page:

![](https://i.imgur.com/9Bv2WKc.png){ width=85% }

The total number of files obtained from the Twitter API is 177.

For our project, we will use files numbers from 67 to 177. These corresponds to tweets from Nov 5 to some tweets from Nov 10.

# Tweets Preprocessing 

The CSV files containing the Twitter data were uploaded to Github. 

The following function creates a vector with all links to be accessed to retrieve data.

```{r}
start_url <- "https://raw.githubusercontent.com/bsosnovski/FinalProject/master/tweet"
end_url <- ".csv"

# the selected files to be used in this project
vec <- seq(67,177)

# function
pages <- function(vec){
        n <- length(vec)
        urls <- vector('character')
        for (i in 1:n){
                temp <- str_c(start_url,vec[i],end_url, collapse = "")
                urls <- c(urls, temp)
        }
        return(urls)
}

urls <-pages(vec)
head(urls)

```

The urls created will be used to open connections to the files, read them into data frames, select the columns of interest and bind the data frames.

```{r}
n <-length(urls)
Stream <-data.frame()

for (i in 1:n){
        csvfile <- url(urls[i])
        df <- read.csv(csvfile,header = TRUE, fileEncoding = "ASCII", stringsAsFactors = FALSE)
        df <- df %>% select(results.created_at,results.text,results.user.name,results.user.location)
        Stream <- rbind(Stream,df)
}

str(Stream)
```


For our analysis, we are interested in the tweets sent by people other than the candidates themselves, so we exclude the tweets from candidates' usernames.

```{r}
Stream <- Stream %>% filter(!results.user.name %in% c("Max Rose","Dan Donovan"))

```

We also change data format to make analysis easier.

```{r}
# Change format of dates
Stream$results.created_at <- as.POSIXct(Stream$results.created_at, format = "%a %b %d %H:%M:%S +0000 %Y")
str(Stream)
```

# Tweets Cleaning
 
The next steps are to cleanse the texts in the tweets:
 
* Remove ASCII symbols and Twitter's user handles (@user)
* Remove punctuations, numbers, digits and special characters 
* Remove white spaces and stop words
* Remove hashtags, tags, urls, Twitter short words, etc.
* Convert the corpus to lower case


```{r}
# Filtering off the retweets from the data (keep only original posts)
Stream <- Stream %>% 
  filter(!str_detect(results.text, "^RT"))

Mycorpus <- Corpus(VectorSource(Stream$results.text))

#Various cleansing funtions:
#ASCII Symbols
remove_ASCIIs <- function(x) gsub("[^\x01-\x7F]", "", x)
Mycorpus <- suppressWarnings(tm_map(Mycorpus, content_transformer(remove_ASCIIs)))

#@'s 
remove_ATs <- function(x) gsub("@\\w+", "", x)
Mycorpus <- suppressWarnings(tm_map(Mycorpus, content_transformer(remove_ATs)))

#All Punctuations
remove_Puncts <- function(x) gsub("[[:punct:]]", "", x)
Mycorpus <- suppressWarnings(tm_map(Mycorpus, content_transformer(remove_Puncts)))

#All Digits
remove_Digits <- function(x) gsub("[[:digit:]]", "", x)
Mycorpus <- suppressWarnings(tm_map(Mycorpus, content_transformer(remove_Digits)))

#3-Step HTTP Process
remove_HTTPSs <- function(x) gsub("http\\w+", "", x)
Mycorpus <- suppressWarnings(tm_map(Mycorpus, content_transformer(remove_HTTPSs)))
remove_HTTPSs2 <- function(x) gsub("[ \t]{2,}", "", x)
Mycorpus <- suppressWarnings(tm_map(Mycorpus, content_transformer(remove_HTTPSs2)))
remove_HTTPSs3 <- function(x) gsub("^\\s+|\\s+$", "", x)
Mycorpus <- suppressWarnings(tm_map(Mycorpus, content_transformer(remove_HTTPSs3)))

#Whitespaces
remove_WhiteSpace <- function(x) gsub("[ \t]{2,}", "", x)
Mycorpus <- suppressWarnings(tm_map(Mycorpus, content_transformer(remove_WhiteSpace)))

#Lower Case
Mycorpus <- suppressWarnings(tm_map(Mycorpus, content_transformer(tolower)))

#im's 
remove_IMs <- function(x) gsub("im", "", x)
Mycorpus <- suppressWarnings(tm_map(Mycorpus, content_transformer(remove_IMs)))

#stopwards
Mycorpus <- suppressWarnings(tm_map(Mycorpus, removeWords,stopwords("English")))

# View the corpus
inspect(Mycorpus[1:10])
```

We take a look at the word cloud with the terms in the corpus.

```{r}
#setting the same seed each time ensures consistent look across clouds
set.seed(7)
suppressWarnings(wordcloud(Mycorpus, random.order=F, scale=c(3, 0.5), min.freq = 5, col=rainbow(50)))

```


Because we need to tokenize the text for the analysis, we replace Twitter's texts in original data frame with the clean data from corpus.

```{r}
# Original data frame
head(Stream$results.text, n=10)

# Clean corpus
df <- data.frame(text = get("content", Mycorpus))
head(df, n=10)

Stream$results.text <- as.character(df$text)

# Remove the rows that contain empty strings in the text column after the cleanup
Stream <- Stream %>% filter(results.text !="")

# Add row numbers and move to the front of the data frame
Stream <- Stream %>% mutate(id = row_number()) %>% select(id, everything())

head(Stream, n=10)

# Tokenize the new clean text from the data frame
Streamnew <- Stream %>%  
  unnest_tokens(word, results.text)

```


Looking back at the word cloud, it seems that the stop words from the tm package didn't filtered off all undesired words from the tweets.  So we continue the cleaning further using the stop words from tidytext package.

```{r}
# Remove stop words and other words
data(stop_words)
Streamnew <- Streamnew %>% anti_join(stop_words)
```


# Sentiment Analysis

Here we get the counts of most frequest words found in our data.

```{r}
Streamnew %>%
  count(word, sort = TRUE) %>%
  top_n(20) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(x = word, y = n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip() +
  labs(x = "Count",
       y = "Unique words",
       title = "Count of unique words found in tweets")
```

We are going to use the “bing” sentiment data which classifies words as positive or negative. We are joining the list of words extracted from the tweets with this sentiment data. 

```{r}
# join sentiment classification to the tweet words
bing_word_counts <- Streamnew %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup()
```

The code below creates a plot of positive and negative words. 

```{r}
bing_word_counts %>%
  group_by(sentiment) %>%
  top_n(10) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(title = "Midterm Election Sentiment.",
       y = "Contribution to sentiment",
       x = NULL) +
  coord_flip()
```


Let's take a look at how our results were impacted by time passing, we will take a look at our data at specific times.

The code below adds a column to clasify tweets as "Early, Med, or Late":
- Early: November 5th and 6th
- Med: November 7th
- Late: After Nov 8th

```{r}
Streamnew$Timing <- ifelse(Streamnew$results.created_at <= '2018-11-07', 'Early',
                  ifelse(Streamnew$results.created_at >= '2018-11-07' & Streamnew$results.created_at <= '2018-11-08', 'Med',
                         ifelse(Streamnew$results.created_at >= '2018-11-08', 'Late', 'other')
                  ))
```

The code below joins our data frame with sentiment data and plots positive and negative words in "Early, Med, and Late" timing categories.

```{r}
# join sentiment classification to the tweet words
Elec_sentiment_2018 <- Streamnew %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, Timing, sort = TRUE) %>%
  group_by(sentiment) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  group_by(Timing, sentiment) %>%
  top_n(n = 5, wt = n) %>%
  arrange(Timing, sentiment, n)


Elec_sentiment_2018 %>%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~Timing, scales = "free_y", ncol = 2) +
  labs(title = "Sentiment during the 2018 Midterm Election - NY 11th Cong. District.",
       y = "Number of Times Word Appeared in Tweets",
       x = NULL) +
  coord_flip()
```

# Topic Modeling with LDA

```{r}
dtm <- DocumentTermMatrix(Mycorpus)
dtm
```

The sparse matrix contains rows without entries(words) and this causes error in LDA function. 

To deal with this issue, we compute the sum of words by row and subset the dtm matrix by rows with sum >0.

```{r}
rowTotals <- apply(dtm , 1, sum)
dtm.new   <- dtm[rowTotals> 0, ]    
dtm.new

# Set a seed so that the output of the model is predictable
# Model finds 10 topics
lda <- LDA(dtm.new, k = 4, control = list(seed = 1234))
lda

term <- terms(lda, 10) # first 10 terms of every topic
term

topics <- tidy(lda, matrix = "beta")
topics
```

In the table above, we see that the LDA model computes the probability of that term being generated from that topic. For example the term "followed" has probabilities 0.000152, 0.0000559, 0.000216 and 0.0000935 of being generated from topic 1, 2, 3 and 4, respectively.

The following is a visualization of the results for the top 10 terms.

```{r}
top_terms <- topics %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

top_terms %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip()
```

The four topics appear to be pretty similar. That is not  surprising given that there is a limited number of topics one would expect to see discussed in comments relating to the election results. 

Note that some of the top frequent positive words from the sentiment analysis above such as "congratulations" and "proud" appear in most of the topics. But none of the top negative words appear in the topics' lists.

# Modeling with TF-IDF

The statistic Term Frequency-Inverse Document Frequency (TF-IDF) measures how important a word is to a document in a corpus of documents, as in our case, to one tweet in a collection of tweets.

TF-IDF in general finds:

* Words that are very common in a specific document are probably important to the topic of that document.

* Words that are very common in all documents probably aren't important to the topics of any of them.

So a term will receive a high weight if it's common in a specific document and also uncommon across all documents.

```{r}

# get the count of each word in each individual tweet
mywords <- Stream %>%  
        unnest_tokens(word, results.text) %>% 
        anti_join(stop_words) %>% 
        count(id,word, sort = TRUE) %>% 
        ungroup()

# get the number of words per tweet
total_words <- mywords %>% 
      group_by(id) %>% 
      summarize(total = sum(n))
        
# combine the two dataframes we just made
mywords <- left_join(mywords, total_words)
mywords

# get the tf_idf & order the words by degree of relevence
tf_idf1 <- mywords %>%
      bind_tf_idf(word, id, n) %>%
      select(-total) %>%
      arrange(desc(tf_idf)) %>%
      mutate(word = factor(word, levels = rev(unique(word))))

tf_idf2 <- mywords %>%
      bind_tf_idf(word, id, n) %>%
      select(-total) %>%
      arrange(tf_idf) %>%
      mutate(word = factor(word, levels = rev(unique(word))))

print(tf_idf1, n=10)

print(tf_idf2, n=10)

```

If the TF-IDF is closer to zero, it indicates that the word is extrememly common. Thus, the words "staten", "island" and "ny" are not so important for the whole corpus. On the other hand, the words with high TD-IDF are important. As one can see, they have high due to typos and concatenation of words together. Nevertheless, note that majority of them are included some of the most frequent positive terms of the sentiment analysis.

# Conclusions



# Reference

* Jeff Gentry. "twitteR - Twitter client for R." March 18, 2014. R package version 1.1.9. <https://www.rdocumentation.org/packages/twitteR/versions/1.1.9>

* hupseb. "How to use premium API for the first time (beginner)?" Post #10, May 13, 2018. Twitter Developers Forums. <https://twittercommunity.com/t/how-to-use-premium-api-for-the-first-time-beginner/105346/10>

* Julia Silge and David Robinson. "Text Mining with R. A Tidy Approach." Sep 23, 2018. <https://www.tidytextmining.com/index.html>

* Hadley Wickham. "Best practices for API packages." Aug 20, 2017. R package httr Vignette. <https://cran.r-project.org/web/packages/httr/vignettes/api-packages.html>

* Leah Wasser, Carson Farmer. "Lesson 6. Sentiment Analysis of Colorado Flood Tweets in R"
https://www.earthdatascience.org/courses/earth-analytics/get-data-using-apis/sentiment-analysis-of-twitter-data-r/
