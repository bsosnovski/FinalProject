---
title: "Final Project: Midterm Election Results - Sentiment Analysis Using Twitter's API"
author: "B. Sosnovski, E. Azrilyan and R. Mercier"
date: "11/23/2018"
output:
  prettydoc::html_pretty:
    theme: architect
    highlight: github
    section: TRUE
---

# Project Description: 

Sentiment analysis on the primary election results using Twitter data.

**Members:** B. Sosnovski, E. Azrilyan and R. Mercier

**Motivation:** Sentiment analysis plays an important role during elections. Political strategists can use the public’s opinions, emotions, attitudes, etc., to convert them into votes in 2020.
Can we use the public’s sentiment to determine which political party has the upper hand in the next election?
Data: To conduct our analysis we will harvest data using Twitter and Facebook’s APIs. Data will be restricted to a certain date range and geographic region. 

**Work Flow:** 

1. Fetch, clean and tokenize the data.
2. Perform feature selection to keep only the n-grams (most likely bigrams) that are meaningful for an analysis.
3. Classify results as positive, negative and neutral. At this phase of the project, we not sure yet what type of analysis to perform. A possible analysis that can be performed is one of the  following:

   + Counting and correlating pairs of words.
   + Build predictive models using logistic regression to predict the probability of occurrence of an event.
   + Probabilistic topic model using Latent Dirichlet Allocation (LDA) method to be applied for reading general tendency from FB/Twitter posts or comments into certain topics that can be classified toward positive and negative sentiment.
   
**Tools:**

+ Tweeter Premium Search API - 30-day endpoint (Sandbox), which provides tweets from the previous 30 days.
+ R Packages

# Twitter Premium API

First we need Twitter API Access. The following steps were taken to set up a twitter account and be able to use the Twitter API.

1. Created a twitter account.
2. Logged in with the twitter credentials on <https://dev.twitter.com/apps> and applied for a developer account.
3. After receiving approval from Twitter, submitted an application to create new app, filled out the form and agreed to the terms.
4. Created the Keys and Access Tokens.

![Twitter Dashboard for Developers ](https://i.imgur.com/Sd2QMvj.png){ width=75% }

# Load Libraries
```{r}
library(twitteR)
library(httr)
library(base64enc)
library(jsonlite)
library(stringr)
library(dplyr)
library(knitr)
library(XML)
library(RCurl)
library(methods)
library(tm)
library(wordcloud)
```

# API Credentials

Read key, key secret, access token and and access token secret from a text file to mantain the information confidential.

```{r eval=TRUE}
api <- read.table("Twitter_API_Key.txt", header = TRUE, stringsAsFactors = FALSE)
names(api)
dim(api)
App_Name <- api$app_name
Consumer_Key <- api$key
Consumer_Secret <- api$secret_Key
Access_Token <- api$access_token
Access_Secret <- api$access_token_secret
```

The following two chunks of code were retrieved from <https://twittercommunity.com/t/how-to-use-premium-api-for-the-first-time-beginner/105346/10>

# API Authentication

```{r eval=FALSE}
# base64 encoding
kands <- paste(Consumer_Key, Consumer_Secret, sep=":")
base64kands <- base64encode(charToRaw(kands))
base64kandsb <- paste("Basic", base64kands, sep=" ")

# request bearer token
resToken <- POST(url = "https://api.twitter.com/oauth2/token",
                 add_headers("Authorization" = base64kandsb, "Content-Type" = "application/x-www-form-urlencoded;charset=UTF-8"),
                 body = "grant_type=client_credentials")

# get bearer token
bearer <- content(resToken)
bearerToken <- bearer[["access_token"]]
bearerTokenb <- paste("Bearer", bearerToken, sep=" ")
```

# Data Acquisition

Since the Twitter Premium API - Sandbox (free version) limits access to the tweets posted the last 30 days only, it is important to save the results of the search in to csv files. This way, we can have access to the results afterwards.

When converting the data received from the API  to a data frame, it may include columns with lists as observations. In this case, the R function "write.csv"" returns an error. The function below identifies which columns of the data frame contain lists. Such columns  The information removed in the process includes not important for our project.   

```{r}
# Function to identify which columns are lists
list_col <- function(df){
        n <- length(df)
        vec <- vector('numeric')
        for (i in 1:n){
                cl <- df[,i]
                if(class(cl)=="list"){
                        vec <- c(vec,i)
                }
        }
        return(vec)
}

```

Requests for data will likely generate more data than can be returned in a single response (our limit is up to 100 tweets in a single response). When a response is paginated, the API response will respond with a "Next" token specified in the body that indicates whether any further pages are available. These "next page tokens" can then be used to make further requests.The following function automates the API requests with or without pagination. 

```{r eval=FALSE}

# the query includes terms "#maxrose", "#dandonovan", "@RepDanDonovan", "@MaxRose4NY" 
# data range: from 2018/11/11 to 2018/11/19

sbody = "{\"query\": \"#maxrose OR #dandonovan OR @RepDanDonovan OR @MaxRose4NY\",\"maxResults\": 100, \"fromDate\":\"201811050000\", \"toDate\":\"201811190000"
ebody = "\"}"

request <- function(start_body,end_body){
         full_body <- str_c(start_body, end_body, sep = "")
         nxt <-""
         pageNo <- 1
         
         while(!is.null(nxt)){
                resTweets <- POST(url = "https://api.twitter.com/1.1/tweets/search/30day/dev.json",
                  add_headers("authorization" = bearerTokenb, "content-Type" = "application/json"),
                  body = full_body)
                
                #checking if the type of response is json
                if (http_type(resTweets) != "application/json") {
                        stop("API did not return json", call. = FALSE)}
                
                #checking if the resquest was successful
                if (http_error(resTweets)) {
                        stop(sprintf("Twitter API request failed! Status = %s. Check what went wrong.\n", 
                                     status_code(resTweets)),
                             call. = FALSE)}else{
                                     message("Retrieving page ",pageNo)}
                
                # Parse the data
                tweets_df <- fromJSON(content(resTweets, "text"),flatten = TRUE) %>% data.frame()
                #class(tweets_df)
                #head(tweets_df)
        
                # Saving data only from the texts of the tweets
                text_df <- tweets_df$results.text
                file1 <- str_c("text",pageNo,".csv")
                write.csv(text_df, file1, row.names=FALSE)
        
                # Remove the list-columns
                vec<-list_col(tweets_df)
                tweets_df <- tweets_df[,-vec]

                # Saving the whole data
                #head(tweets_df)
                file2 <- str_c("tweet",pageNo,".csv")
                write.csv(tweets_df, file2, row.names=FALSE)
        
                # Read the "next"" token received in the response
                nxt <- tweets_df$next.[[1]]
        
                if(!is.null(nxt)){
                        # insert the next token in the body of the request
                        full_body <- str_c(start_body, "\", \"next\":\"", nxt, end_body, sep = "")
                        pageNo <- pageNo+1}
                
                # To avoid to exceed the API's limit per minute
                Sys.sleep(3) # to avoid to exceed the API's rate limit per minute
        } #end of while loop
         
}#end of function

request(start_body = sbody,end_body = ebody)
```


# Tweets Preprocessing 

The CSV files containin the Twitter data were uploade to Github. 

The following function that creates a vector with all links to be accessed to retrieve data.

```{r}
start_url <- "https://raw.githubusercontent.com/bsosnovski/FinalProject/master/tweet"
end_url <- ".csv"

# the number of the files to be used in this project
vec <- seq(67,177)

# function
pages <- function(vec){
        n <- length(vec)
        urls <- vector('character')
        for (i in 1:n){
                temp <- str_c(start_url,vec[i],end_url, collapse = "")
                urls <- c(urls, temp)
        }
        return(urls)
}

urls <-pages(vec)
head(urls)

```

The urls created will be used to open connections to the files, read them into data frames, select the columns of interest and bind the data frames.

```{r}
n <-length(urls)
Stream <-data.frame()

for (i in 1:n){
        csvfile <- url(urls[i])
        df <- read.csv(csvfile,header = TRUE, fileEncoding = "ASCII", stringsAsFactors = FALSE)
        df <- df %>% select(results.created_at,results.text,results.user.name,results.user.location)
        Stream <- rbind(Stream,df)
}

str(Stream)
```


For our analysis, we are interest in the tweets sent by people other than the candidates themselves. So we exclude the tweets whose user name are the candidates.

```{r}
Stream <- Stream %>% filter(!results.user.name %in% c("Max Rose","Dan Donovan"))

# Add row numbers and move to the front of the data frame
Stream <- Stream %>% mutate(id = row_number()) %>% select(id, everything())

# Change format of dates
# I don't know if we need to keep it. If so, we replace the original column
Stream$results.created_at <- as.POSIXct(Stream$results.created_at, format = "%a %b %d %H:%M:%S +0000 %Y")
str(Stream)
```

```{r}
# Not sure why to do this so for now I commented this line of code.
#combined_doc <- iconv(Stream, "UTF-8", "ASCII", sub = "")
#str(combined_doc)
```

# Tweets Cleaning
 
The next steps is to  cleanse the texts in the tweets by:
 
* Removing Twitter handles (@user)
* Removing punctuations, numbers, and special characters 
* Removing white spaces (?) and stop words
* Remove hashtags, tags, urls, Twitter short words, etc.
* Converting the corpus to lower case (?)

```{r}
Mycorpus <- Corpus(VectorSource(Stream$results.text))

#Various cleansing funtions:
#ASCII Symbols
remove_ASCIIs <- function(x) gsub("[^\x01-\x7F]", "", x)
Mycorpus <- tm_map(Mycorpus, content_transformer(remove_ASCIIs))

#Retweets
remove_RTs <- function(x) gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", x)
Mycorpus <- tm_map(Mycorpus, content_transformer(remove_RTs))

#@'s 
remove_ATs <- function(x) gsub("@\\w+", "", x)
Mycorpus <- tm_map(Mycorpus, content_transformer(remove_ATs))

#All Punctuations
remove_Puncts <- function(x) gsub("[[:punct:]]", "", x)
Mycorpus <- tm_map(Mycorpus, content_transformer(remove_Puncts))

#All Digits
remove_Digits <- function(x) gsub("[[:digit:]]", "", x)
Mycorpus <- tm_map(Mycorpus, content_transformer(remove_Digits))

#3-Step HTTP Process
remove_HTTPSs <- function(x) gsub("http\\w+", "", x)
Mycorpus <- tm_map(Mycorpus, content_transformer(remove_HTTPSs))
remove_HTTPSs2 <- function(x) gsub("[ \t]{2,}", "", x)
Mycorpus <- tm_map(Mycorpus, content_transformer(remove_HTTPSs2))
remove_HTTPSs3 <- function(x) gsub("^\\s+|\\s+$", "", x)
Mycorpus <- tm_map(Mycorpus, content_transformer(remove_HTTPSs3))

#Whitespaces
remove_WhiteSpace <- function(x) gsub("[ \t]{2,}", "", x)
Mycorpus <- tm_map(Mycorpus, content_transformer(remove_WhiteSpace))

#stopwards
Mycorpus <- tm_map(Mycorpus, removeWords,stopwords("en"))

#Lower Case
Mycorpus <- tm_map(Mycorpus, content_transformer(tolower))


# View the corpus
inspect(Mycorpus[1:10])
```


```{r}
# I don't think we need the dtm below
#dtm <- DocumentTermMatrix(Mycorpus)
suppressWarnings(wordcloud(Mycorpus, random.order=F, scale=c(3, 0.5), min.freq = 5, col=rainbow(50)))

```

#Sentiment Analysis

Replace Twitter's texts in original data frame with the clean data from corpus.

```{r}
# Original data frame
head(Stream, n=10)

# Clean corpus
df <- data.frame(text = get("content", Mycorpus))
head(df, n=10)

Stream$results.text <- as.character(df$text)
head(Stream, n=10)

Streamnew <- Stream %>%  
  unnest_tokens(word, results.text)

```

Here is count of most frequest words, this is similar to what we are showing with a word cloud, so might not be needed.

```{r}
Streamnew %>%
  count(word, sort = TRUE) %>%
  top_n(15) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(x = word, y = n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip() +
  labs(x = "Count",
       y = "Unique words",
       title = "Count of unique words found in tweets")
## Selecting by n
```

We are going to use the “bing” sentiment data which classifies words as positive or negative. We are joining the words extracted from the tweets with this sentiment data. 

```{r}
# join sentiment classification to the tweet words
bing_word_counts <- Streamnew %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup()
## Joining, by = "word"
```

The code below creates a plot of positive and negative words. 

```{r]
bing_word_counts %>%
  group_by(sentiment) %>%
  top_n(10) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(title = "Sentiment Primary Election Sentiment.",
       y = "Contribution to sentiment",
       x = NULL) +
  coord_flip()
```

Let's take a look at our results at different times as time passes.

The code below adds a column to clasify tweets as "Early, Med, or Late"

```{r]
Streamnew$Timing <- ifelse(Streamnew$results.created_at <= '2018-11-07', 'Early',
                  ifelse(Streamnew$results.created_at >= '2018-11-07' & Streamnew$results.created_at <= '2018-11-08', 'Med',
                         ifelse(Streamnew$results.created_at >= '2018-11-08', 'Late', 'other')
                  ))
```

The Code below joins our data frame with sentiment data and plots positive and negative words in those 3 timing categories.

```{r}

# join sentiment classification to the tweet words
Elec_sentiment_2013 <- Streamnew %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, Timing, sort = TRUE) %>%
  group_by(sentiment) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  group_by(Timing, sentiment) %>%
  top_n(n = 5, wt = n) %>%
  arrange(Timing, sentiment, n)
## Joining, by = "word"

Elec_sentiment_2013 %>%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~Timing, scales = "free_y", ncol = 2) +
  labs(title = "Sentiment during the 2013 flood event by month.",
       y = "Number of Times Word Appeared in Tweets",
       x = NULL) +
  coord_flip()


```

# Sentiment Analysis

Check packages SentimentAnalysis and sentimentr.
Check this for a good and simple example of sentiment analysis
<https://www.earthdatascience.org/courses/earth-analytics/get-data-using-apis/sentiment-analysis-of-twitter-data-r/>

# Reference

* Jeff Gentry. "twitteR - Twitter client for R." March 18, 2014. R package version 1.1.9. <https://www.rdocumentation.org/packages/twitteR/versions/1.1.9>

* hupseb. "How to use premium API for the first time (beginner)?" Post #10, May 13, 2018. Twitter Developers Forums. <https://twittercommunity.com/t/how-to-use-premium-api-for-the-first-time-beginner/105346/10>

* Hadley Wickham. "Best practices for API packages". Aug 20, 2017. R package httr Vignette. <https://cran.r-project.org/web/packages/httr/vignettes/api-packages.html>
